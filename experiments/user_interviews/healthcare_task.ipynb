{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The necessary imports and a simple function to plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "def print_dag_operator_histograms(constraint_result, attribute):    \n",
    "    print(\"{} histogram\".format(attribute))\n",
    "            \n",
    "    plt.subplot(1, 2, 1)        \n",
    "    before_output_race_group = constraint_result[attribute].before_map\n",
    "    \n",
    "    # TODO: Move this into check\n",
    "    dict_with_str_keys = {str(key): value for (key, value) in before_output_race_group.items()}\n",
    "    sorted_items = sorted(dict_with_str_keys.items())\n",
    "    before_output_race_group = OrderedDict(sorted_items)\n",
    "    \n",
    "    keys = [str(key) for key in before_output_race_group.keys()]\n",
    "    plt.bar(keys, before_output_race_group.values())\n",
    "    plt.gca().set_title(\"before\")\n",
    "    plt.xticks(\n",
    "        rotation=45, \n",
    "        horizontalalignment='right',\n",
    "    )\n",
    "   \n",
    "    plt.subplot(1, 2, 2)\n",
    "    after_output_race_group = constraint_result[attribute].after_map\n",
    "    \n",
    "    # TODO: Move this into check\n",
    "    dict_with_str_keys = {str(key): value for (key, value) in after_output_race_group.items()}\n",
    "    sorted_items = sorted(dict_with_str_keys.items())\n",
    "    after_output_race_group = OrderedDict(sorted_items)\n",
    "    \n",
    "    keys = [str(key) for key in after_output_race_group.keys()]\n",
    "    plt.bar(keys, after_output_race_group.values())\n",
    "    plt.gca().set_title(\"after\")\n",
    "    plt.xticks(\n",
    "        rotation=45, \n",
    "        horizontalalignment='right',\n",
    "    )\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 4)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description\n",
    "\n",
    "Operators like joins, selections and missing value imputaters can cause *data distribution issues*, which can heavily impact the performance of our model. Mlinspect helps with identifying such issues by offering an inspection to calculate historams for sensitive groups. Thanks to our annotation propagation, this works even if the group columns are projected out at some point. To automatically check for significant changes and compute the histograms, we used the `no_bias_introduced_for(...)` constraint.\n",
    "\n",
    "We want to find out if preprocessing operations in this pipeline introduce bias and if so, which groups are effected.\n",
    "The pipeline we want to analyse can be found using the path `os.path.join(str(get_project_root()), \"example_pipelines\", \"healthcare\", \"healthcare.py\")`. Its code:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "An example pipeline\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from example_pipelines.healthcare.healthcare_utils import MyW2VTransformer, create_model\n",
    "from mlinspect.utils import get_project_root\n",
    "\n",
    "# FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "COUNTIES_OF_INTEREST = ['county2', 'county3']\n",
    "\n",
    "# load input data sources (data generated with https://www.mockaroo.com as a single file and then split into two)\n",
    "patients = pd.read_csv(os.path.join(str(get_project_root()), \"example_pipelines\", \"healthcare\",\n",
    "                                    \"healthcare_patients.csv\"), na_values='?')\n",
    "histories = pd.read_csv(os.path.join(str(get_project_root()), \"example_pipelines\", \"healthcare\",\n",
    "                                     \"healthcare_histories.csv\"), na_values='?')\n",
    "\n",
    "# combine input data into a single table\n",
    "data = patients.merge(histories, on=['ssn'])\n",
    "\n",
    "# compute mean complications per age group, append as column\n",
    "complications = data.groupby('age_group').agg(mean_complications=('complications', 'mean'))\n",
    "\n",
    "data = data.merge(complications, on=['age_group'])\n",
    "\n",
    "# target variable: people with a high number of complications\n",
    "data['label'] = data['complications'] > 1.2 * data['mean_complications']\n",
    "\n",
    "# project data to a subset of attributes\n",
    "data = data[['smoker', 'last_name', 'county', 'num_children', 'race', 'income', 'label']]\n",
    "\n",
    "# filter data\n",
    "data = data[data['county'].isin(COUNTIES_OF_INTEREST)]\n",
    "\n",
    "# define the feature encoding of the data\n",
    "impute_and_one_hot_encode = Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encode', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "featurisation = ColumnTransformer(transformers=[\n",
    "    (\"impute_and_one_hot_encode\", impute_and_one_hot_encode, ['smoker', 'county', 'race']),\n",
    "    ('word2vec', MyW2VTransformer(min_count=2), ['last_name']),\n",
    "    ('numeric', StandardScaler(), ['num_children', 'income'])\n",
    "])\n",
    "\n",
    "# define the training pipeline for the model\n",
    "neural_net = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0, input_dim=109)\n",
    "pipeline = Pipeline([\n",
    "    ('features', featurisation),\n",
    "    ('learner', neural_net)])\n",
    "\n",
    "# train-test split\n",
    "train_data, test_data = train_test_split(data, random_state=0)\n",
    "# model training\n",
    "model = pipeline.fit(train_data, train_data['label'])\n",
    "# model evaluation\n",
    "print(model.score(test_data, test_data['label']))\n",
    "```\n",
    "\n",
    " This is using a synthetic benchmark dataset we created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1/3: Add inspections and execute the pipeline\n",
    "\n",
    "The central entry point of mlinspect is the `PipelineInspector`. To use mlinspect, we use it and pass it the path to the runnable version of the example pipeline. Here, we have the example pipeline in a `.py` file. \n",
    "\n",
    "First, we define the check we want mlinspect to run. In this example, we only use 1 check to compute histograms of sensitive groups and verify operators cause no significant distribution changes.\n",
    "\n",
    "Then, we execute the pipeline. Mlinspect returns a `InspectorResult`, which contains the output of our check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlinspect.utils import get_project_root\n",
    "\n",
    "from mlinspect import PipelineInspector\n",
    "from mlinspect.checks import NoBiasIntroducedFor\n",
    "\n",
    "HEALTHCARE_FILE_PY = os.path.join(str(get_project_root()), \"example_pipelines\", \"healthcare\", \"healthcare.py\")\n",
    "\n",
    "inspector_result = PipelineInspector\\\n",
    "    .on_pipeline_from_py_file(HEALTHCARE_FILE_PY) \\\n",
    "    .add_check(NoBiasIntroducedFor([\"age_group\", \"race\"])) \\\n",
    "    .execute()\n",
    "\n",
    "check_results = inspector_result.check_to_check_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2/3: Overview of the check results\n",
    "## Did our check find issues?\n",
    "\n",
    "Let us look at the `check_result` to see whether some constraints failed. We see that some constraints failed and investigate which ones. As all 3 failed, we will look into each result in detail.\n",
    "\n",
    "Our check already filtered all operators that can cause data distribution issues. Now we will use the result to create list with all distribution changes and visualize them. Using this, we can investigate the changes of the different operators one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bias_check_result = check_results[NoBiasIntroducedFor([\"age_group\", \"race\"])]\n",
    "print(\"CheckResult NoBiasIntroduced: {}\".format(no_bias_check_result.status))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Dag nodes that could have changed the distribution:\")\n",
    "distribution_changes = []\n",
    "for dag_node, distribution_change in no_bias_check_result.bias_distribution_change.items():\n",
    "    print('\\033[1m')\n",
    "    print(dag_node.operator_type, dag_node.code_reference, dag_node.module, dag_node.description, '\\033[0m')\n",
    "    distribution_changes.append(distribution_change)\n",
    "    for column, change_info in distribution_change.items():\n",
    "        if not change_info.acceptable_change:\n",
    "            print(\"Sensitive column '{}': \\033[1mThe distribution changed too much!\\033[0m\".format(column))\n",
    "        else:\n",
    "            print(\"Sensitive column '{}': No distribution change above the test threshold.\".format(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3/3: Detailed Investigation\n",
    "\n",
    "As we can see, the selection causes the check to fail because of the `race` attribute. Still, we will investigate all of the operator changes to see if there is something else our check may have missed because the change was slightly below the change threshold of the `NoBiasIntroducedFor([\"race\", \"sex\"])` (which can be configured by the user).\n",
    "\n",
    "## Let's see how different operations might change proportions of groups in data\n",
    "\n",
    "We start by looking at the first operator that could heavily change the proportion of groups in our data, the `.dropna()` selection. Then, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = distribution_changes[2]\n",
    "print_dag_operator_histograms(selection, \"age_group\")\n",
    "print_dag_operator_histograms(selection, \"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, there are no noteworthy changes because of the join.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `most-frequent` imputation amplifies the existing `race` imbalance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer: Did we find operators that introduce bias? How much did the distribution change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude, gender bias can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
